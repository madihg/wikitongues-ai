# Cultural Language Benchmark Report

**Epoch:** epoch_1  
**Date:** 2026-02-17 22:00 UTC  
**Languages:** Igala, Lebanese Arabic  
**Models:** Chatgpt, Claude, Gemini, Gemma

## Executive Summary

- **Igala**: Claude leads with an overall rubric mean of 4.38/5.
- **Lebanese Arabic**: Claude leads with an overall rubric mean of 4.17/5.
- **Overall pairwise winner**: Claude (71% win rate across all matchups).

## Pairwise Win Rates

### Igala

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | 0% | 50% | 100% |
| **Claude** | 100% | - | 0% | 100% |
| **Gemini** | 50% | 100% | - | -- |
| **Gemma** | 0% | 0% | -- | - |

### Lebanese Arabic

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | 50% | -- | -- |
| **Claude** | 50% | - | 100% | -- |
| **Gemini** | -- | 0% | - | 100% |
| **Gemma** | -- | -- | 0% | - |

### Win Rate Confidence Intervals (95%)

| Model | Overall Win % | 95% CI |
|---|---|---|
| **Chatgpt** | 43% | [14%, 86%] |
| **Claude** | 71% | [29%, 100%] |
| **Gemini** | 67% | [33%, 100%] |
| **Gemma** | 0% | [0%, 0%] |

## Rubric Scores

### Igala

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | 3.00 +/- 0.00 | 2.67 +/- 0.50 | 3.33 +/- 0.50 | 3.67 +/- 0.50 | 3.17 +/- 0.33 |
| **Claude** | 4.75 +/- 0.38 | 3.75 +/- 0.38 | 4.25 +/- 0.38 | 4.75 +/- 0.38 | 4.38 +/- 0.31 |
| **Gemini** | 4.00 +/- 0.00 | 3.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.50 +/- 0.50 |
| **Gemma** | 2.00 +/- 0.00 | 1.50 +/- 0.50 | 2.00 +/- 0.00 | 2.50 +/- 0.50 | 2.00 +/- 0.38 |

### Lebanese Arabic

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | 3.50 +/- 0.50 | 3.50 +/- 0.50 | 3.50 +/- 0.50 | 4.50 +/- 0.50 | 3.75 +/- 0.50 |
| **Claude** | 4.33 +/- 0.50 | 4.33 +/- 0.50 | 4.00 +/- 1.00 | 4.00 +/- 0.00 | 4.17 +/- 0.33 |
| **Gemini** | 3.50 +/- 0.50 | 3.50 +/- 0.50 | 3.50 +/- 0.50 | 4.00 +/- 0.00 | 3.62 +/- 0.31 |
| **Gemma** | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 3.00 +/- 0.00 | 2.25 +/- 0.38 |

## Breakdown by Prompt Category

### Abstract vs Everyday

**Pairwise Win Rates:**

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | 100% | -- | -- |
| **Claude** | 0% | - | -- | -- |
| **Gemini** | -- | -- | - | 100% |
| **Gemma** | -- | -- | 0% | - |

**Rubric Scores:**

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 5.00 +/- 0.00 | 4.25 +/- 0.38 |
| **Claude** | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.75 +/- 0.38 |
| **Gemini** | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.50 +/- 0.50 |
| **Gemma** | -- | -- | -- | -- | -- |

### Frontier Aspirations

**Pairwise Win Rates:**

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | -- | -- | 100% |
| **Claude** | -- | - | -- | 100% |
| **Gemini** | -- | -- | - | -- |
| **Gemma** | 0% | 0% | -- | - |

**Rubric Scores:**

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | -- | -- | -- | -- | -- |
| **Claude** | 5.00 +/- 0.00 | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 5.00 +/- 0.00 | 4.50 +/- 0.50 |
| **Gemini** | -- | -- | -- | -- | -- |
| **Gemma** | 2.00 +/- 0.00 | 1.00 +/- 0.00 | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 1.75 +/- 0.38 |

### Real-World Use

**Pairwise Win Rates:**

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | 0% | -- | -- |
| **Claude** | 100% | - | 50% | -- |
| **Gemini** | -- | 50% | - | -- |
| **Gemma** | -- | -- | -- | - |

**Rubric Scores:**

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | 3.00 +/- 0.00 | 2.67 +/- 0.50 | 3.33 +/- 0.50 | 3.67 +/- 0.50 | 3.17 +/- 0.33 |
| **Claude** | 4.50 +/- 0.50 | 4.00 +/- 0.75 | 4.50 +/- 0.50 | 4.25 +/- 0.38 | 4.31 +/- 0.28 |
| **Gemini** | 4.00 +/- 0.00 | 3.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.50 +/- 0.50 |
| **Gemma** | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 3.00 +/- 0.00 | 2.25 +/- 0.38 |

### Words & Concepts

**Pairwise Win Rates:**

| | Chatgpt | Claude | Gemini | Gemma |
|---|---|---|---|---|
| **Chatgpt** | - | -- | 50% | -- |
| **Claude** | -- | - | -- | -- |
| **Gemini** | 50% | -- | - | 100% |
| **Gemma** | -- | -- | 0% | - |

**Rubric Scores:**

| Model | Cultural Accuracy | Linguistic Authenticity | Creative Depth | Factual Correctness | Overall |
|---|---|---|---|---|---|
| **Chatgpt** | 3.00 +/- 0.00 | 3.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 3.25 +/- 0.38 |
| **Claude** | 5.00 +/- 0.00 | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 5.00 +/- 0.00 | 4.50 +/- 0.50 |
| **Gemini** | 4.00 +/- 0.00 | 3.00 +/- 0.00 | 4.00 +/- 0.00 | 4.00 +/- 0.00 | 3.75 +/- 0.38 |
| **Gemma** | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 2.00 +/- 0.00 | 3.00 +/- 0.00 | 2.25 +/- 0.38 |

## Inter-Annotator Agreement

| Dimension | Krippendorff's alpha | Interpretation |
|---|---|---|
| Cultural Accuracy | 0.583 | Moderate |
| Linguistic Authenticity | 0.571 | Moderate |
| Creative Depth | 0.083 | Low |
| Factual Correctness | 0.167 | Low |
| Pairwise Selection | 0.000 | Low |

**Overall rubric alpha (mean across dimensions):** 0.351 (Low)

## Methodology

This benchmark evaluates large language models on culturally grounded prompts across multiple languages. Evaluation combines two approaches:

1. **Pairwise comparison**: Human annotators compare outputs from two models side-by-side and select the better response, with a written explanation. Win rates are computed per model pair.
2. **Rubric scoring**: Each model output is scored on a 1-5 scale across four dimensions: Cultural Accuracy, Linguistic Authenticity, Creative Depth, and Factual Correctness.

Inter-annotator agreement is measured using Krippendorff's alpha -- ordinal scale for rubric scores and nominal scale for pairwise selections. Confidence intervals are computed via bootstrap resampling (2000 iterations, 95% CI).
